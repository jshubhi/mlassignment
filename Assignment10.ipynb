{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmQJThqLdQpy",
        "colab_type": "code",
        "outputId": "0f347f75-19f1-4212-ea15-e1cec79356a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Question 1\n",
        "import random\n",
        "m = (0.2,0.4,0.1,0.1,0.1,0.1)\n",
        "def roll(m):\n",
        "    randRoll = random.random()   # inrange[0,1]\n",
        "    sum = 0\n",
        "    result = 1\n",
        "    for n in m:\n",
        "        sum += n\n",
        "        if randRoll < sum:\n",
        "            return result\n",
        "        result+=1\n",
        "\n",
        "def dice(p):\n",
        "  step = 0;\n",
        "  numstep = 0;\n",
        "  for i in range(100000):\n",
        "    for i in range(250):\n",
        "      result = roll(p)\n",
        "      #dice rolled \n",
        "      if result==1 or result==2: step=max(0,step-1)\n",
        "      elif result>=3 and result<=5: step=step+1\n",
        "      else: \n",
        "        result1 = roll(p)\n",
        "        step=step+result1\n",
        "    if step>60: numstep=numstep+1\n",
        "  return numstep\n",
        "\n",
        "numstep = dice(m)\n",
        "print(\"Probability= \",numstep/100000)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probability =  0.27917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE2S4At4wi2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Question 2\n",
        "from sklearn.datasets import make_regression\n",
        "from matplotlib import pyplot\n",
        "\n",
        "X1, y1 = make_regression(n_samples=100, n_features=3, noise=0.2)  # regression dataset generated\n",
        "\n",
        "pyplot.scatter(X1[:,0]+X1[:,1]+X1[:,2],y1)           #plot dataset\n",
        "pyplot.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9vMkosJArYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot\n",
        "from sklearn.datasets import make_blobs\n",
        "from pandas import DataFrame\n",
        "\n",
        "X2, y2 = make_blobs(n_samples=100, centers=2, n_features=2)\n",
        "\n",
        "df = DataFrame(dict(x=X2[:,0], y=X2[:,1], label=y2))\n",
        "colors = {0:'green', 1:'blue'} #dots colored by class value in scatter plot\n",
        "fig, ax = pyplot.subplots()\n",
        "grouped = df.groupby('label')\n",
        "for key, group in grouped:\n",
        "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uEzABW7DppA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot\n",
        "from pandas import DataFrame\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X3, y3 = make_blobs(n_samples=100, centers=3, n_features=2)\n",
        "\n",
        "df = DataFrame(dict(x=X3[:,0], y=X3[:,1], label=y3))\n",
        "colors = {0:'pink', 1:'orange', 2:'green'}\n",
        "fig, ax = pyplot.subplots()\n",
        "grouped = df.groupby('label')\n",
        "for key, group in grouped:\n",
        "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM1_uLu3ET6z",
        "colab_type": "text"
      },
      "source": [
        "## Question-3(a)[OOPS]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I38zL_nyEGlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LinearRegression():\n",
        "    def __init__(self, X, y, alpha=0.03, n_iter=1500):\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.n_iter = n_iter\n",
        "        self.n_samples = len(y)\n",
        "        self.n_features = np.size(X, 1)\n",
        "        self.X = np.hstack((np.ones(\n",
        "            (self.n_samples, 1)), (X - np.mean(X, 0)) / np.std(X, 0)))\n",
        "        self.y = y[:, np.newaxis]\n",
        "        self.params = np.zeros((self.n_features + 1, 1))\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "\n",
        "    def fit(self):\n",
        "\n",
        "        for i in range(self.n_iter):\n",
        "            self.params = self.params - (self.alpha/self.n_samples) * \\\n",
        "            self.X.T @ (self.X @ self.params - self.y)\n",
        "\n",
        "        self.intercept_ = self.params[0]\n",
        "        self.coef_ = self.params[1:]\n",
        "\n",
        "        return self\n",
        "\n",
        "    def score(self, X=None, y=None):\n",
        "\n",
        "        if X is None:\n",
        "            X = self.X\n",
        "        else:\n",
        "            n_samples = np.size(X, 0)\n",
        "            X = np.hstack((np.ones(\n",
        "                (n_samples, 1)), (X - np.mean(X, 0)) / np.std(X, 0)))\n",
        "\n",
        "        if y is None:\n",
        "            y = self.y\n",
        "        else:\n",
        "            y = y[:, np.newaxis]\n",
        "\n",
        "        y_pred = X @ self.params\n",
        "        score = 1 - (((y - y_pred)**2).sum() / ((y - y.mean())**2).sum())\n",
        "\n",
        "        return score\n",
        "\n",
        "    def predict(self, X):\n",
        "        n_samples = np.size(X, 0)\n",
        "        y = np.hstack((np.ones((n_samples, 1)), (X-np.mean(X, 0)) \\\n",
        "                            / np.std(X, 0))) @ self.params\n",
        "        return y\n",
        "\n",
        "    def get_params(self):\n",
        "\n",
        "        return self.params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH_byfUcGO8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import LinearRegression as lr\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "X = X1\n",
        "y = y1\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\\\n",
        "                X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "our_regressor = LinearRegression(X_train, y_train).fit()\n",
        "\n",
        "our_train_accuracy = our_regressor.score()\n",
        "\n",
        "our_test_accuracy = our_regressor.score(X_test, y_test)\n",
        "\n",
        "pd.DataFrame([[our_train_accuracy],\n",
        "              [our_test_accuracy]],\n",
        "             ['Training Accuracy', 'Test Accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SIR5MH3HNSX",
        "colab_type": "text"
      },
      "source": [
        "## 3(b)[oops]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzzsMuCEG45l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True):\n",
        "        self.lr = lr\n",
        "        self.num_iter = num_iter\n",
        "        self.fit_intercept = fit_intercept\n",
        "    \n",
        "    def __add_intercept(self, X):\n",
        "        intercept = np.ones((X.shape[0], 1))\n",
        "        return np.concatenate((intercept, X), axis=1)\n",
        "    \n",
        "    def __sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def __loss(self, h, y):\n",
        "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        if self.fit_intercept:\n",
        "            X = self.__add_intercept(X)\n",
        "        \n",
        "        # weights initialization\n",
        "        self.theta = np.zeros(X.shape[1])\n",
        "        \n",
        "        for i in range(self.num_iter):\n",
        "            z = np.dot(X, self.theta)\n",
        "            h = self.__sigmoid(z)\n",
        "            gradient = np.dot(X.T, (h - y)) / y.size\n",
        "            self.theta -= self.lr * gradient\n",
        "    \n",
        "    def predict_prob(self, X):\n",
        "        if self.fit_intercept:\n",
        "            X = self.__add_intercept(X)\n",
        "    \n",
        "        return self.__sigmoid(np.dot(X, self.theta))\n",
        "    def predict(self, X, threshold):\n",
        "        return self.predict_prob(X) >= threshold\n",
        "    def plot(self):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='b', label='0')\n",
        "        plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='r', label='1')\n",
        "        plt.legend()\n",
        "        x1_min, x1_max = X[:,0].min(), X[:,0].max(),\n",
        "        x2_min, x2_max = X[:,1].min(), X[:,1].max(),\n",
        "        xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
        "        grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
        "        probs = model.predict_prob(grid).reshape(xx1.shape)\n",
        "        plt.contour(xx1, xx2, probs, [0.5], linewidths=1, colors='black');\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyoh-19C6zoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gd = GenerateData()\n",
        "X, y = gd.logisticRegre()\n",
        "Y = Y.reshape(-1,1)# This will reshape Y as a column vector. conversally reshape(1,-1) will reshape an array as row vector\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\\\n",
        "                X, y, test_size=0.3, random_state=42)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "model.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJsA8kIdJ7vK",
        "colab_type": "text"
      },
      "source": [
        "##3(c)-oops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdSB2pPiJ_N8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import statistics as st\n",
        "plt.rcParams['figure.figsize'] = (5.0, 5.0)\n",
        "\n",
        "class RidgeLinearRegression:\n",
        "       def __init__(self,X_data,Y_data,l,l2,iterations): #initialize all parameters\n",
        "           self.X=X_data\n",
        "           self.Y=Y_data\n",
        "           #weight and bias\n",
        "           self.m=0\n",
        "           self.c=0\n",
        "           self.L=l #learning rate\n",
        "           self.l2=l2 #regularization parameter\n",
        "           self.iter=iterations #num of iterations\n",
        "           self.n=float(len(self.X))  #size of data\n",
        "       \n",
        "       def cost(self,pred_y): #cost function\n",
        "           cost=np.sum(np.square(self.Y-pred_y))/(2*self.n) + self.l2*np.sum(np.square(self.m))\n",
        "           return(cost)\n",
        "       \n",
        "       def fit(self):\n",
        "           self.history=np.zeros(self.iter)\n",
        "           #updating values of m and c\n",
        "           for i in range(self.iter):\n",
        "               pred_y=self.m*self.X + self.c\n",
        "               #print(pred_y)\n",
        "               Dm= (-2/self.n)*(self.X*(self.Y-pred_y))+2*self.l2*self.m\n",
        "               Dc= (-2/self.n)*(self.Y-pred_y)\n",
        "               #update\n",
        "               self.m=self.m-Dm*self.L\n",
        "               self.c=self.c-Dc*self.L\n",
        "               #cost is calculated for every iteration\n",
        "               self.history[i]=self.cost(pred_y)\n",
        "           self.mse=self.MSE(self.Y,pred_y)\n",
        "               \n",
        "               \n",
        "               \n",
        "      \n",
        "       def MSE(self,pred_y,Y):\n",
        "           errors=Y-pred_y #error is the difference between actual and predicted value\n",
        "           mse=np.sum(np.square(errors))/self.n #mean of sum of square of erros\n",
        "           return mse\n",
        "       \n",
        "       def results(self):\n",
        "           fig=plt.figure(figsize=(14,14))\n",
        "           a1=fig.add_subplot(211)\n",
        "\n",
        "           plt.title('minimisation of errors across the iterations')\n",
        "           a1.plot(self.history)\n",
        "\n",
        "\n",
        "           #making predictions\n",
        "           a2=fig.add_subplot(212)\n",
        "           final_y=self.m*self.X +self.c \n",
        "           plt.scatter(self.X,self.Y)\n",
        "           plt.title('regrssion line')\n",
        "           a2.plot([min(self.X),max(self.X)],[min(final_y),max(final_y)],color='red') #plotting the red line \n",
        "           \n",
        "           plt.show()\n",
        "           \n",
        "           print ('Mean Squared Error=',self.mse)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGxZvHf4gFZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l=RidgeLinearRegression(X1[:,0],y1,0.005,0.001,100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtt32eFZg1gs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l.fit()\n",
        "l.results()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvYTVmGHg9LG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import statistics as st\n",
        "plt.rcParams['figure.figsize'] = (5.0, 5.0)\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "class LassoLinearRegression:\n",
        "       def __init__(self,X_data,Y_data,l,l1,iterations): #initialize all parameters\n",
        "           self.X=X_data\n",
        "           self.Y=Y_data\n",
        "           #weight and bias\n",
        "           self.m=np.random.randn(X_data.shape[0])\n",
        "           self.c=0\n",
        "           self.L=l #learning rate\n",
        "           self.l1=l1 #regularization parameter\n",
        "           self.iter=iterations #num of iterations\n",
        "           self.n=float(len(X_data))  #size of data\n",
        "       \n",
        "       def cost(self,pred_y): #cost function\n",
        "           cost=np.sum(np.square(self.Y-pred_y))/(2*self.n) + self.l1*np.sum(self.m)\n",
        "           return(cost)\n",
        "       \n",
        "       def fit(self):\n",
        "           self.history=np.zeros(self.iter)\n",
        "           pred_y=np.zeros((self.Y.shape))\n",
        "           for j in range(self.iter):\n",
        "               for i in range(0,len(self.X)):\n",
        "                       pred_y[i]=self.m[i]*self.X[i]+self.c\n",
        "                       if self.m[i]>0:\n",
        "                           Dm= (-2/self.n)*(self.X[i]*(self.Y[i]-pred_y[i]))-self.l1 \n",
        "                       else:\n",
        "                           Dm= (-2/self.n)*(self.X[i]*(self.Y[i]-pred_y[i]))+self.l1\n",
        "\n",
        "\n",
        "                       Dc= (-2/self.n)*(self.Y[i]-pred_y[i])\n",
        "                       #update\n",
        "                       self.m[i]=self.m[i]-Dm*self.L\n",
        "                       self.c=self.c-Dc*self.L\n",
        "               self.history[j]=self.cost(pred_y)\n",
        "           self.mse=self.MSE(self.Y,pred_y)\n",
        "\n",
        "             \n",
        "      \n",
        "       def MSE(self,pred_y,Y):\n",
        "           errors=Y-pred_y #error is the difference between actual and predicted value\n",
        "           mse=np.sum(np.square(errors))/self.n #mean of sum of square of erros\n",
        "           return mse\n",
        "       \n",
        "       def results(self):\n",
        "           fig=plt.figure(figsize=(14,14))\n",
        "           a1=fig.add_subplot(211)\n",
        "\n",
        "           plt.title('minimisation of errors across the iterations')\n",
        "           a1.plot(self.history)\n",
        "\n",
        "\n",
        "           #making predictions\n",
        "           a2=fig.add_subplot(212)\n",
        "           final_y=self.m*self.X +self.c \n",
        "           plt.scatter(self.X,self.Y)\n",
        "           plt.title('regrssion line')\n",
        "           a2.plot([min(self.X),max(self.X)],[min(final_y),max(final_y)],color='red') #plotting the red line \n",
        "           \n",
        "           plt.show()\n",
        "           \n",
        "           print ('Mean Squared Error=',self.mse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AovvZMVtiL3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l=LassoLinearRegression(X1[:,0],y1,0.005,0.001,100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JukCOXzuiQWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l.fit()\n",
        "l.results()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjTdeMFI490j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Question 3d\n",
        "import numpy as np\n",
        "# import tensorflow\n",
        "# import utils\n",
        "import math\n",
        "import scipy \n",
        "from scipy import optimize\n",
        "import random\n",
        "# from touvlo import utils\n",
        "\n",
        "class RegLogisticRegressor:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.theta = None\n",
        "\n",
        "    def train(self,X,y,var,reg=1e-5,num_iters=400,norm=True):\n",
        "        num_train,dim = X.shape\n",
        "\n",
        "        # standardize features if norm=True\n",
        "        if norm:\n",
        "            # take out the first column and do the feature normalize\n",
        "            X_without_1s = X[:,1:]\n",
        "            X_norm, mu, sigma = self.std_features(X_without_1s)\n",
        "            # add the ones back\n",
        "            XX = np.vstack([np.ones((X_norm.shape[0],)),X_norm.T]).T\n",
        "        else:\n",
        "            XX = X\n",
        "\n",
        "        # initialize theta\n",
        "        theta = np.zeros((dim,))\n",
        "\n",
        "        # Run scipy's fmin algorithm to run gradient descent\n",
        "        if var == \"L1\":\n",
        "            theta_opt_norm = scipy.optimize.fmin_bfgs(self.lossL1, theta, fprime = self.grad_loss, args=(XX,y,reg),maxiter=num_iters)\n",
        "        else:\n",
        "            theta_opt_norm = scipy.optimize.fmin_bfgs(self.lossL2, theta, fprime = self.grad_loss, args=(XX,y,reg),maxiter=num_iters)\n",
        "\n",
        "\n",
        "\n",
        "        if norm:\n",
        "            # convert theta back to work with original X\n",
        "            theta_opt = np.zeros(theta_opt_norm.shape)\n",
        "            theta_opt[1:] = theta_opt_norm[1:]/sigma\n",
        "            theta_opt[0] = theta_opt_norm[0] - np.dot(theta_opt_norm[1:],mu/sigma)\n",
        "        else:\n",
        "            theta_opt = theta_opt_norm\n",
        "\n",
        "\n",
        "        return theta_opt\n",
        "    def lossL1(self, *args):\n",
        "        theta,X,y,reg = args\n",
        "        m,dim = X.shape\n",
        "        J = 0\n",
        "\n",
        "        J = ((-np.array([y])).dot(np.log(sigmoid(X.dot((np.array([theta])).T))))[0,0]-(1-np.array([y])).dot(np.log(1-sigmoid(X.dot((np.array([theta])).T))))[0,0])/m + reg*np.array(([theta]))[0,0]/(2*m)\n",
        "\n",
        "        return J\n",
        "\n",
        "    def lossL2(self, *args):\n",
        "        theta,X,y,reg = args\n",
        "        m,dim = X.shape\n",
        "        J = 0\n",
        "\n",
        "        J = ((-np.array([y])).dot(np.log(sigmoid(X.dot((np.array([theta])).T))))[0,0]-(1-np.array([y])).dot(np.log(1-sigmoid(X.dot((np.array([theta])).T))))[0,0])/m + reg*np.array([theta]).dot(np.array([theta]).T)[0,0]/(2*m)\n",
        "\n",
        "        return J\n",
        "\n",
        "    def grad_loss(self, *args):\n",
        "        theta,X,y,reg = args\n",
        "        m,dim = X.shape\n",
        "        grad = np.zeros((dim,))\n",
        "        grad = ((sigmoid((np.array([theta])).dot(X.T))-np.array([y])).dot(X))[0]/m\n",
        "        grad[1:] = grad[1:] + reg*theta[1:]/m\n",
        "\n",
        "        return grad\n",
        "        \n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.zeros(X.shape[0])\n",
        "\n",
        "        y_pred = sigmoid((np.array([self.theta])).dot(X.T))[0]\n",
        "\n",
        "        return y_pred  \n",
        "    def sigmoid (self, z):\n",
        "        sig = np.zeros(z.shape)\n",
        "        # Your code here\n",
        "        sig = 1/(1+np.exp(-z))\n",
        "        # End your ode\n",
        "\n",
        "        return sig\n",
        "\n",
        "    def log_features(self, X):\n",
        "        logf = np.zeros(X.shape)\n",
        "        # Your code here\n",
        "        logf = np.log(X+0.1)\n",
        "        # End your ode\n",
        "        return logf\n",
        "\n",
        "    def std_features(self, X):\n",
        "        mu = np.mean(X,axis=0)\n",
        "        sigma = np.std(X,axis=0)\n",
        "        X_norm = (X - mu) / sigma\n",
        "        return X_norm, mu, sigma\n",
        "\n",
        "    def bin_features(self, X):\n",
        "        tX = np.zeros(X.shape)\n",
        "        # your code here\n",
        "        tX[X>0] = 1\n",
        "        # end your code\n",
        "        return tX \n",
        "    def select_lambda_crossval(self, X,y,lambda_low,lambda_high,lambda_step,penalty):\n",
        "      \n",
        "        \n",
        "        best_lambda = lambda_low\n",
        "\n",
        "        # Your code here\n",
        "        # Implement the algorithm above.\n",
        "\n",
        "        lambda_test = lambda_low\n",
        "        highest_accuracy =0.0\n",
        "        \n",
        "        while(lambda_test<=lambda_high):       \n",
        "            sk_logreg_l1 = linear_model.LogisticRegression(C=1.0/lambda_test,solver='liblinear',fit_intercept=False,penalty='l1')\n",
        "            sk_logreg_l2 = linear_model.LogisticRegression(C=1.0/lambda_test,solver='sag',fit_intercept=False)       \n",
        "            if (penalty == \"l1\"):\n",
        "                sk_logreg = sk_logreg_l1\n",
        "            elif (penalty == \"l2\"):\n",
        "                sk_logreg = sk_logreg_l2\n",
        "            else:\n",
        "                print (\"error in penalty\")  \n",
        "                break                     \n",
        "            kf = cross_validation.KFold(X.shape[0], n_folds = 10)        \n",
        "            cumulate_accuracy = 0.0\n",
        "            for train_set,test_set in kf:\n",
        "                X_train, X_test = X[train_set], X[test_set]\n",
        "                y_train,y_test = y[train_set],y[test_set]\n",
        "                sk_logreg.fit(X_train,y_train)\n",
        "                #print \"Theta found by sklearn with \",penalty,\" regularization \" ,sk_logreg.coef_[0]\n",
        "                y_predict = sk_logreg.predict(X_test)\n",
        "                sub_accuracy = 1- np.nonzero(np.round(y_predict - y_test))[0].size/float(y_test.shape[0])\n",
        "                #print \"The accuracy is \", sub_accuracy\n",
        "                cumulate_accuracy = cumulate_accuracy+sub_accuracy\n",
        "                #print \"cumulate accuracy is \", cumulate_accuracy\n",
        "            accuracy = cumulate_accuracy/10\n",
        "            if(accuracy>highest_accuracy):\n",
        "                highest_accuracy = accuracy\n",
        "                best_lambda = lambda_test        \n",
        "            lambda_test = lambda_test + lambda_step\n",
        "        return best_lambda            \n",
        "        \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Td0X5pmF6G-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = gd.logisticRegre()\n",
        "Y = Y.reshape(-1,1)# This will reshape Y as a column vector. conversally reshape(1,-1) will reshape an array as row vector\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\\\n",
        "                X, y, test_size=0.3, random_state=42)\n",
        "model = RegLogisticRegressor()\n",
        "model.train(X_train, y_train, 'L1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9EdMq6z6KLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gd = GenerateData()\n",
        "X, y = gd.logisticRegre()\n",
        "Y = Y.reshape(-1,1)# This will reshape Y as a column vector. conversally reshape(1,-1) will reshape an array as row vector\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\\\n",
        "                X, y, test_size=0.3, random_state=42)\n",
        "model = RegLogisticRegressor()\n",
        "model.train(X_train, y_train, 'L2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEbWhTlVia2m",
        "colab_type": "text"
      },
      "source": [
        "## 3(e)-oops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9Vi0uvDifYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in data:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGNzI-joryaU",
        "colab_type": "code",
        "outputId": "c3703f5a-186a-4f4f-bfed-e48cb7862b96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# X3\n",
        "clf = K_Means()\n",
        "clf.fit(X3)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.6129593952455483\n",
            "310.74190270399663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS_iN-k6s63u",
        "colab_type": "code",
        "outputId": "e149c42d-49d2-47fc-c35b-c2152e7eab32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "correct = 0\n",
        "for i in range(len(X3)):\n",
        "    predict_me = np.array(X3[i].astype(float))\n",
        "    predict_me = predict_me.reshape(-1, len(predict_me))\n",
        "    prediction = clf.predict(predict_me)\n",
        "    if prediction == y3[i]:\n",
        "        correct += 1\n",
        "print(correct/len(X3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU-QSGuKtcQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}